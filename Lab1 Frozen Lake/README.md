# Лабораторная работа №1 на тему «Табулярное Q-обучение игрового агента Toy text»

## Задание
1. В среде Python (3.6 и старше) создать проект и подключить библиотеку Gym.
2. Прочитать, перевести, включить в отчёт и повторить эксперимент из документа FrozenLake-v0. 
Подготовить видеофайл с эпизодом игры.
3. Дать в отчёте описание игры, среды и агента из Игры в соответствии с вариантом по аналогии с п.2:
https://gym.openai.com/envs/#toy_text
4. Повторить эксперимент из п. 2 для Игры в соответствии с Вариантом.
5. Исследовать влияние параметров LEARNING_RATE, DISCOUNT_FACTOR, TOTAL_EPISODES на исход игры. Обосновать выбор лучших
параметров.
6. Провести сравнительный эксперимент по определению количества побед и скорости обучения для своей игры с помощью
агента обученного табулярным Q-learning и случайно действующим агентом (random).
7. Подготовить и защитить отчёт (титульный лист, задание, теоретическая часть, графики экспериментов, диаграмма
структуры программы, принтскрины основных шагов работы программы, листинг программы с комментариями, список
использованной литературы + 2 видеофайла).

# Состав проекта
В `lab1_origin.py` находится код, присланный преподавателем.  
В `frozen_lake_agents.py` определены классы агентов для сред типа FrozenLake.  
В `default.py` содержится скрипт, создающий агента, запускающий его обучение и тестирующий обученного агента подсчётом
винрейта. Этот скрипт можно запустить из командной строки следующим образом:
```
python default.py [environment_name]
```
Так как при запуске этого скрипта в IDE PyCharm отображение обучения и тестирования происходит некорректно,
был написан скрипт `default.bat`. Он в командной строке активирует виртуальную среду и запускает `default.py`.  
В `parameters_exploring.py` содержится скрипт, который исследует влияние параметров LEARNING_RATE, DISCOUNT_FACTOR,
TOTAL_EPISODES на исход игры.
